---
title: 常用内置模块
order: 3

tag:
  - Node
  - 内置模块
  - API
---

## path

path 模块用于对路径和文件进行处理，提供了很多好用的方法。

主要因为 Mac OS、Linux 和 windows 上的路径写法不一样

在 window 上会使用 \或者 \\ 来作为文件路径的分隔符，当然目前也支持 /

在 Mac OS、Linux 的 Unix 操作系统上使用 / 来作为文件路径的分隔符

为了屏蔽他们之间的差异，在开发中对于路径的操作我们可以使用 path 模块

### 可移植操作系统接口

Portable Operating System Interface，缩写为 POSIX

Linux 和 Mac OS 都实现了 POSIX 接口；

Window 部分电脑实现了 POSIX 接口；

### 获取路径信息

dirname：获取文件的父文件夹；

basename：获取文件名；

extname：获取文件扩展名；

### 路径拼接

如果我们希望将多个路径进行拼接，但是不同的操作系统可能使用的是不同的分隔符

这时候可以使用 path.join 函数

### 将文件和某个文件夹拼接

如果我们希望将某个文件和文件夹拼接，可以使用 path.resolve;

resolve 函数会判断我们拼接的路径前面是否有 /或../或./；

如果有表示是一个绝对路径，会返回对应的拼接路径；如果没有，那么会和当前执行文件所在的文件夹进行路径的拼接

在 webpack 中获取路径或者起别名的地方也可以使用。

## fs

### File System 文件系统

借助于 Node 帮我们封装的文件系统，我们可以在任何的操作系统（window、Mac OS、Linux）上面直接去操作文件；

这也是 Node 可以开发服务器的一大原因，也是它可以成为前端自动化脚本等热门工具的原因。

Node 的 API 大多数都提供三种操作方式：

方法 1：同步操作文件：代码会被阻塞，不会继续执行；

方法 2：异步回调函数操作文件：代码不会被阻塞，需要传入回调函数，当获取到结果时，回调函数被执行；

方法 3：异步 Promise 操作文件：代码不会被阻塞，通过 fs.promises 调用方法操作，会返回一个 Promise，可以通过 then、catch 进行处理。

### File Descriptors 文件描述符

在 POSIX 系统上，对于每个进程，内核都维护着一张当前打开着的文件和资源的表格。

每个打开的文件都分配了一个称为文件描述符的简单的数字标识符。

在系统层，所有文件系统操作都使用这些文件描述符来标识和跟踪每个特定的文件。

Windows 系统使用了一个虽然不同但概念上类似的机制来跟踪资源。

最终，为了简化用户的工作，Node.js 抽象出操作系统之间的特定差异，并为所有打开的文件分配一个数字型的文件描述符。

### fs.open()

用于分配新的文件描述符。一旦被分配，则文件描述符可用于从文件读取数据、向文件写入数据、或请求关于文件的信息。

```js
fs.open("../foo.txt", "r", (err, fd) => {
  console.log(fd);
  fs.fstat(fd, (err, state) => {
    console.log(state);
  });
});
```

- 参数 1：文件路径，String 类型

- 参数 2：文件打开模式 options.flag 'r' 'w' ...

- 参数 3：callback 回调函数

详细描述见 Node 官方文档

### fs 读写

fs.readFile(path[, options], callback)：读取文件的内容；

fs.writeFile(file, data[, options], callback)：在文件中写入内容；

#### options.flag

w 打开文件写入，默认值；

w+打开文件进行读写，如果不存在则创建文件；

r+ 打开文件进行读写，如果不存在那么抛出异常；

r 打开文件读取，读取时的默认值；

a 打开要写入的文件，将流放在文件末尾。如果不存在则创建文件；

a+打开文件以进行读写，将流放在文件末尾。如果不存在则创建文件

#### options.encoding

最常用 UTF-8 编码

文件读取 readFile 中，如果不填写 encoding，返回的结果是 Buffer。

### fs 操作文件夹

fs.mkdir() fs.mkdirSync() 创建新文件夹

#### 读取文件夹内容

```js
fs.readdir(dirname, (err, files) => {
  console.log(files);
});
```

将目录中读取到的文件名称转为 files 数组

#### 文件夹重命名

```js
fs.rename("../old", "../new", (err) => {
  console.log(err);
});
```

#### 文件夹复制

```js
fs.copyFile(srcFile, destFile);
```

## Buffer

### 服务器的处理

比如某一个保存文本的文件并不是使用 utf-8 进行编码的，而是用 GBK，那么我们必须读取到他们的二进制数据，再通过 GBK 转换成对应的文字；

比如我们需要读取的是一张图片数据（二进制），再通过某些手段对图片数据进行二次的处理（裁剪、格式转换、旋转、添加滤镜），Node 中有一个 Sharp 的库，就是读取图片或者传入图片的 Buffer 对其再进行处理；

比如在 Node 中通过 TCP 建立长连接，TCP 传输的是字节流，我们需要将数据转成字节再进行传入，并且需要知道传输字节的大小（客服端需要根据大小来判断读取多少内容）

### Buffer 与字符串

Buffer 相当于是一个字节的数组，数组中的每一项对于一个字节的大小

### Buffer.alloc

Buffer.alloc( number ) 创建一个指定长度的 Buffer，所有数据默认为 00

### Buffer 和文件读取

fs.readFile 读取完成后 callback 的 data 的默认数据类型就是 Buffer

### Buffer 创建过程

事实上我们创建 Buffer 时，并不会频繁的向操作系统申请内存，它会默认先申请一个 8 \* 1024 个字节大小的内存，
也就是 8kb

Buffer.from→Buffer.fromString→Buffer.fromStringFast

查看 fromStringFast 源码

这里做的事情是判断剩余的长度是否还足够填充这个字符串

如果不足够，那么就要通过 createPool 创建新的空间

如果够就直接使用，但是之后要进行 poolOffset 的偏移变化

## stream

### stream 为何高效

要想搞清楚这个问题，我们先来了解下缓冲模式与流模式的概念。

缓冲模式(buffer mode)：在这种模式下系统会把传来的所有数据都先放到缓冲区里，直到数据传递结束为止，然后系统把这些数据当成一个整块，传给使用方，如图：

![stream缓冲模式图解](https://misaka10032.oss-cn-chengdu.aliyuncs.com/Node/02-%E6%A8%A1%E5%9D%97%E5%8C%96/stream1.webp)

在 t1 时刻系统收到了数据 Hello n ，到了 T2 时刻系统又收到了最后一个数据块 ode，于是整个读取完成了，在 t3 时刻系统会把整个缓冲区的数据发送给消费者。

与缓冲模式相反，在流模式下 系统只要从生产者获取到数据就立刻发送给消费者，让它能够尽快处理数据， 如图：

![stream流模式图解](https://misaka10032.oss-cn-chengdu.aliyuncs.com/Node/02-%E6%A8%A1%E5%9D%97%E5%8C%96/stream2.webp)

从图中可以看出在流模式下，系统会把收到的每一块数据立即传送给消费方，让消费者有机会立刻处理数据，而不像缓冲模式那样必须收集到所有数据然后才发送，从效率上来看，流模式在空间上和时间上都要比缓冲模式强，试想一个操作超大文件的场景，缓冲模式会因为读取大量数据而程序崩溃，但是 stream 模式不会，他有一套完善的数据流控制，申请的缓冲区如果填满来不及消费，可以控制传输速度，从而完成对超大文件的处理操作。除此之外流模式还有一个优势-便于组合，通过 pipe 可以连接不同的 stream 对数据进行处理，后面会详细介绍。

### stream 模块体系结构

nodejs 平台里的每一种对象都属于下面这四个基本抽象类中的一个，这些类是由 stream 核心模块提供的，**每个 stream 类对象都是一个 [EventEmitter](#events) 实例**，所以流对象上可以触发一些特定的事件，如 Readable 流在读取完毕时会出发 end 事件，Writable 流在写入完毕后会触发 finish 事件，如果操作过程发生异常会触发 error 事件。

- Writable
- Readable
- Duplex
- Transform

stream 很灵活，它不仅可以处理二进制数据，且几乎可以处理任何一种 javascript 值，这是因为流对象的操作模式分为两种：

- 二进制模式：以块(chunk)的形式串流数据，这种模式的好处是可以用来处理 Buffer 或字符串;
- 对象模式：以对象序列的形式串流数据，这意味着我们几乎能处理任何一种 Javascript 数据类型;

由于 stream 对象能支持这样两种操作模式，因此我们不仅可以用它来处理 I/O，而且还能像函数式编程那样，把各种处理

### Writable 可写流

在 Node 中，Writable 这个抽象类位于 stream 模块中，Node 中也有不少接口就是实现的这个抽象类

- fs 模块中的 createWriteStream 方法
- 客户端上的 HTTP 请求对象
- 服务器上的 HTTP 响应对象
- 压缩流 zlib
- 加密流 crypto
- TCP 套接字 net.Socket
- 子进程标准输入 subprocess.stdin
- process.stdout、process.stderr

当然除了使用上面这些系统已经实现的 Writable 可写流，我们也可以实现一个自定义的 Writable 流，要实现自定义的流，我们需要继承 Writable 抽象类，然后在自己的这个类中给\_write 方法提供实现代码:

```js
const { Writable } = require("stream");

class MyWritable extends Writable {
  constructor(options) {
    // 调用 stream.Writable() 构造函数。
    super(options);
    // ...
  }

  _write(chunk, encoding, callback) {
    // TODO
  }
}

// 如果要实现的流比较简单，也可以直接这样写，注意没有_
const myWritable = new Writable({
  write(chunk, encoding, callback) {
    // TODO
  },
  // ...其他options
});

const writeSream = new myWritable(options);
```

自定义可写流 demo：

```js
// toFileStream.js demo 实现一个往指定路径中写入一个文件的自定义Writable流
const { Writable } = require("stream");
const { promises: fs } = require("fs");
const { dirname } = require("path");
const mkdirp = require("mkdirp-promise");

export class ToFileStream extends Writable {
  constructor(options) {
    super({ ...options, objectMode: true });
  }

  _write(chunk, encoding, cb) {
    mkdirp(dirname(chunk.path))
      .then(() => fs.writeFile(chunk.path, chunk.content))
      .then(() => cb())
      .catch(cb);
  }
}

// index.js
const { join } = require("path");
const { ToFileStream } = require("./toFileStream.js");
const tfs = new ToFileStream();

tfs.write({ path: join("files", "file1.txt"), content: "Hello" });
tfs.write({ path: join("files", "file2.txt"), content: "Node.js" });
tfs.write({ path: join("files", "file3.txt"), content: "streams" });
tfs.end(() => console.log("All files created"));
```

跟真实的管道一样，node 的流也可能会出现拥堵，即有可能会出现写入速度大于数据的消费速度，为了应对这种情况，流对象会把写进来的数据先放入缓冲区，但如果给流写入数据的那个人不知道已经出现了拥堵，他还是不断的往流里写数据，这会导致缓冲区数据越来越多，使内存占用飙升，为了让写入方知道这种情况，write 方法会在内部缓冲区达到 highWaterMark 上限的时候返回 false，表示不应该再向其中写入数据，此时可以监听一下流的 drain 事件， 当缓冲区清空时，流对象会触发 drain 事件以提示可以向流中写入数据了。

```js
// 是否可写
const drain = writeSream.write("Hello node.js...");
if (!drain) {
  writeSream.once("drain", writeSream.write("other data..."));
} else {
  writeSream.write("other data...");
}
```

当然，这套机制只是建议机制， 不强制用户必须实施，即使我们不去实施还是可以继续往流里面写入数据的，但是实际开发中我们最好进行背压，避免拥堵导致内存飙升。

### Readable 可读流

Readable 流同样位于 stream 模块，表示的数据源，node 中也有不少已实现的可读流

- 客户端上的 HTTP 响应对象
- 服务器上的 HTTP 请求对象
- 文件系统读取流 fs.createReadaStream
- 压缩流 zlib
- 加密流 crypto
- TCP 套接字 net.Socket
- 子进程的标准输出和标准错误 subprocess.stdout
- process.stdin

可以发现其中的很多也是 Writable 流，这些重复的是后面要介绍的 Duplex 双工流（可读/可写流），它们既实现了 Readable 抽象类也实现了 Writable 抽象类。

想通过 Readable 流来读取数据，有两种方法：

1. 非流动模式（non-flowing），也叫做 pause 模式，这是默认模式。
2. 流动模式（flowing）。

非流动模式是我们从 Readable 流中读取数据的默认模式， 在这种模式下我们可以监听 readable 事件，一旦触发就说明有新的数据可以读取了，我们通过一个循环结构反复读取数据，读取方法就是 read 方法，这个方法会从内部缓冲区同步的读出数据，并返回数据或者 null，null 表示没有数据可读，该方法接受一个可选的参数 size，用于指定要读取的数据量，如果不传则返回缓冲区的所有数据。

```js
// 把终端的的内容读取出来，并将读到的数据回显到终端
process.stdin
  .on("readable", () => {
    let chunk;
    console.log("New data available");
    while ((chunk = process.stdin.read()) !== null) {
      console.log(`Chunk read (${chunk.length} bytes): "${chunk.toString()}"`);
    }
  })
  .on("end", () => console.log("End of stream"));
```

注意 read 方法返回的数据格式是通过 setEncoding 方法来设置的，如果在二进制模式下则返回的是 Buffer 对象，如果设置有效的编码方式如 utf8，就可以不用读取 BUffer 了，直接读到字符串，而且 setEncoding 这个方法可以在读取过程中更换编码方式，stream 内部会自动切换，流内部数据本身并没有编码一说，本质上都是二进制，指定编码只是按照某一套标准去解析而已。

如果想启用流动模式只需要监听 data 事件即可，或者明确的调用 resume 方法，对于已经进入流动模式的流如果暂时不想他推送 data 事件可以调用 pause 方法，stream 会把拿到的数据缓存到缓冲区，这个方法会让流切换为非流动模式，流动模式下我们通过监听 data 事件来获取流推送过来的数据，无需再调用 read 方法读取数据

```js
// 功能同上
process.stdin
  .on("data", (chunk) => {
    console.log("New data available");
    console.log(`Chunk read (${chunk.length} bytes): "${chunk.toString()}"`);
  })
  .on("end", () => console.log("End of stream"));
```

除了使用平台已实现的可读流， 我们同样可以实现一个自定义的可读流，与 Writable 类似，首先继承 Readable 抽象类，然后在自定义的类中实现\_read 方法， 这个方法的实现中我们需要通过 this.push 向内部缓冲区推送数据

```js
// 伪代码
const { Readable } = require('stream');

class MyReadable extends Readable {
  constructor(options) {
    // 调用 stream.Readable() 构造函数。
    super(options);
    // ...
  }

  _read (size) {
    // 生成供流使用者使用的数据
    TODO...
    // 推送到内部缓冲区以供读取
    this.push(someData);
  }
}

// 如果要实现的流比较简单，也可以直接这样写，注意没有_
const myReadable = new Readable({
  read (size) {
    // 生成供流使用者使用的数据
    TODO...
    // 推送到内部缓冲区以供读取
    this.push(someData);
  }
  // ...其他options
})

const readSream = new MyReadable(options);

// demo.js 一个可以自动生成随机字符串的可读流
import { Readable } from 'stream'
// chance模块用于生成随机串，npm包
import Chance from 'chance'

const chance = new Chance()

export class RandomStream extends Readable {
  constructor (options) {
    super(options)
    this.emittedBytes = 0
  }
  // 此处size是建议只推入调用方请求的这么多字节，只是建议，不是强制，所以我们还是要检查下push方法返回的是true还是false
  // false 表示推入的数据已经达到了highWaterMark表示的上限，你不应该再往里面写数据了，这也是背压，和上Writable中的背压类似
  _read (size) {
    const chunk = chance.string({ length: size }) // ①
    // 推入的是字符串所以设置编码方式
    this.push(chunk, 'utf8') // ②
    this.emittedBytes += chunk.length
    // 让这个流有5%的概率可以终止
    if (chance.bool({ likelihood: 5 })) { // ③
      // push(null)会给内部缓冲区写入EOF(文件结束)符，表示这个stream流到此结束
      this.push(null)
    }
  }
}
```

### Duplex 双工流（可读/可写流）

Duplex 流既是 Readable 流，又是 Writable 流，他可以用来描述那些既充当数据来源，又充当数据目标的实体，例如网络套接字（network socket），它继承了 stream 模块的 Readable 和 Writable 流，这两种流前面已经详细介绍过，你只需要明白 Readable 和 Writable 流有的方法它都有，而且它也同样可以自定义，如果需要自定义一个 Duplex 流，那么你既需要实现\_read 方法，也需要实现\_write 方法，它的构造函数 options 还有如下参数

- `allowHalfOpen <boolean>` 如果设为 false，则流将在可读端结束时自动结束可写端， 默认值: true。
- `readable <boolean>` 设置 Duplex 是否可读， 默认值: true。
- `writable <boolean>` 设置 Duplex 是否可写， 默认值: true。
- `readableObjectMode <boolean>` 为流的可读端设置 objectMode， 如果设置了 objectMode 是 true，则无效。 默认值: false。
- `writableObjectMode <boolean>` 为流的可写端设置 objectMode，如果设置了 objectMode 是 true，则无效。 默认值: false。
- `readableHighWaterMark <number>` 为流的可读端设置 highWaterMark， 如果提供 highWaterMark，则无效。
- `writableHighWaterMark <number>` 为流的可写端设置 highWaterMark， 如果提供 highWaterMark，则无效。

### Transform 流

Transform 流是一种特殊的 Duplex 流，专门用来转换数据的，对于普通的 Duplex 流来说读取数据与写入数据之间没有直接联系以 TCP socket 为例，它只需要知道自己可以给远端发数据，并从远端接受数据就可以了，至于接收到的数据与发送出去的数据之间是什么关系，它并不需要关心。Transform 流则不一样，它会对对自己从 Writable 端接收到的数据进行转换，并让用户可以从 Readable 端读取到转换后的数据，它们之间的关系可以用下图表示

![transform流](https://misaka10032.oss-cn-chengdu.aliyuncs.com/Node/02-%E6%A8%A1%E5%9D%97%E5%8C%96/transform%E6%B5%81.webp)

尽管 Transform 流是一种特殊的 Duplex 流，但是它的自定义却不像普通 Duplex 流那样实现\_read 和\_write 方法，他需要实现的是另一组方法\_transform 和\_flush。

```js
import { Transform } from "stream";

// 该流实现了将数据中的某个字符全部替换成另一个字符串
export class ReplaceStream extends Transform {
  constructor(searchStr, replaceStr, options) {
    super({ ...options });
    this.searchStr = searchStr;
    this.replaceStr = replaceStr;
    this.tail = "";
  }

  _transform(chunk, encoding, callback) {
    // 拿到数据进行转换
    const pieces = (this.tail + chunk).split(this.searchStr);
    const lastPiece = pieces[pieces.length - 1];
    const tailLen = this.searchStr.length - 1;
    this.tail = lastPiece.slice(-tailLen);
    pieces[pieces.length - 1] = lastPiece.slice(0, -tailLen);
    // 如同readable中的_read里的push一样，将转换后的数据推送到缓冲区以供读取端读取转换后的数据
    this.push(pieces.join(this.replaceStr));
    // 通知当前chunk已转换完毕 可以处理下一段了
    callback();
  }

  _flush(callback) {
    this.push(this.tail);
    callback();
  }
}
```

通过阅读上面代码可以发现，整条数据流结束时，我们可能还有一点数据在 tail 变量里，没来的急推送到内部缓冲区供读取端读取，这正是\_flush 的作用，系统会在流即将结束时出发这个方法，让我们有机会将还没推送完的数据一次推送进去。

### pipe 管道连接流

看下面一条命令， 这条命令先让 echo 程序把 Hello World！输出到标准输出端， 然后通过管道符（｜）把这个输出端重定向到 sed 程序的标准输入端，sed 程序会把里面的 World 替换成 Nodejs。

```
echo Hello World! | sed s/World/Nodejs/g
```

node 中也有类似的管道操作，比如你可以通过下面这个 pipe 方法将 Readable 流与 Writable 流连接起来

```js
readable.pipe(writable, [options]);
```

这个接口的意思是将 readable 流产生的数据推送给 writable 流，当 readable 流出发 end 事件的时候，系统会让 writable 流也结束（除非你给 options 传递了{end: false}）, 这个方法会把它收到的第一个参数返回给调用方，所以如果这个 writable 流如果同时也是一个 Readable 流，那么调用者就可以继续在这个流上面调用 pipe 方法，从而形成链式调用，这样通过 pipe 连接的流是不需要我们再去进行背压处理的，因为内部会自动处理，看下面这个 demo 演示了 pipe 的链式调用

```js
import { request } from "http";
import { createGzip } from "zlib";
import { createReadStream } from "fs";
import { basename } from "path";
import { createCipheriv, randomBytes } from "crypto";

const filename = process.argv[2];
const serverHost = process.argv[3];
const secret = Buffer.from(process.argv[4], "hex");

const iv = randomBytes(16);

const httpRequestOptions = {
  hostname: serverHost,
  port: 3000,
  path: "/",
  method: "PUT",
  headers: {
    "Content-Type": "application/octet-stream",
    "Content-Encoding": "gzip",
    "X-Filename": basename(filename),
    "X-Initialization-Vector": iv.toString("hex"),
  },
};

const req = request(httpRequestOptions, (res) => {
  console.log(`Server response: ${res.statusCode}`);
});

createReadStream(filename)
  .pipe(createGzip())
  .pipe(createCipheriv("aes192", secret, iv))
  .pipe(req)
  .on("finish", () => {
    console.log("File successfully sent");
  });
```

这里链式调用虽然很简洁，但其实并不健壮，因为用 pipe 方法连接的时候前一个流产生的错误不会自动传播给下一个流，看下面这段代码

```js
stream1
  .pipe(stream2)
  .on('error' error => {
    // TODO
  })
```

这段代码只能捕获到 stream2 的异常，无法捕获到 stream1 的异常，我们需要这样处理

```js
stream1
  .on('error', error => {
    // TODO
  })
  .pipe(stream2)
  .on('error' error => {
    // TODO
  })
```

这样写显然比较啰嗦，如果我们的管道里有很多处理环节，这样的写法会变得很糟糕。更严重的问题是，当某个流对象发生错误的时候， 系统会将这个流从管道移除， 如果此时我们没有正确的去销毁管道中的其他流，那么就有可能让某些资源处于游离状态，例如可能导致文件描述符或网络连接未能及时关闭，从而导致内存泄漏，下面的代码可以让代码健壮一点，但是依然不太理想

```js
function handle(error) {
  console.error(error);
  stream1.destrory();
  stream2.destrory();
}

stream1.on("error", handle).pipe(stream2).on("error", handle);
```

因此 stream 中还提供了一个 pipeline 方法，让我们可以更简单的处理管道中的异常，这个方法会将各种流串联起来的同时还会正确的处理其中的异常，并在异常发生后正确的销毁相关的流，看下面的写法，是不是简单了很多

```js
pipeline(stream1, stream2, (error) => {
  if (error) {
    console.error(error);
  } else {
    // 管道执行完成
  }
});
```

## Events

Node 中的核心 API 都是基于异步事件驱动的

某些对象（发射器（Emitters））发出某一个事件；

监听这个事件（监听器 Listeners），并且传入的回调函数，这个回调函数会在监听到事件时调用；

发出事件和监听事件都是通过**EventEmitter 类**来完成的，它们都属于 events 对象。

`emitter.on(eventName, listener)`：监听事件，也可以使用 addListener；

`emitter.off(eventName, listener)`：移除事件监听，也可以使用 removeListener；

`emitter.emit(eventName[, ...args])`：发出事件，可以携带一些参数；

### 常见属性

`emitter.eventNames()`： 返回当前 EventEmitter 对象注册的事件字符串数组；

`emitter.getMaxListeners()`： 返回当前 EventEmitter 对象的最大监听器数量，可以通过 setMaxListeners()来修改，默认是 10；

`emitter.listenerCount(事件名称)`： 返回当前 EventEmitter 对象某一个事件名称，监听器的个数；

`emitter.listeners(事件名称)`：返回当前 EventEmitter 对象某个事件监听器上所有的监听器数组。

### 方法补充

`emitter.once(eventName, listener)`：事件监听一次

`emitter.prependListener()`：将监听事件添加到最前面

`emitter.prependOnceListener()`：将监听事件添加到最前面，但是只监听一次

`emitter.removeAllListeners([eventName])`：移除所有的监听器
